# Task_08_Bias_Detection

# ğŸï¸ Task 08 â€“ Bias Detection in LLM Data Narratives (F1 Dataset)

This repository implements a controlled experiment to detect **biases in Large Language Model (LLM)**â€“generated data narratives.  
It follows the structure of **Research Task 08** (Bias Detection in LLM Data Narratives) under Syracuse Universityâ€™s research framework.

---

## ğŸ“˜ Objective
To test whether identical datasets produce **different analyses from LLMs** when the **prompt framing** or **demographic context** changes.  
The experiment investigates:
- **Framing Bias:** â€œunderperformingâ€ vs â€œdevelopingâ€
- **Demographic Bias:** inclusion of nationalities or protected traits
- **Confirmation Bias:** testing primed hypotheses (e.g., â€œFerrari has the strongest strategyâ€)
- **Selection Bias:** which statistics the model emphasizes (wins, DNFs, pit stops, etc.)

---

## ğŸ§© Dataset
We use anonymized Formula 1 performance data from the **2022 season**, derived from:
- `drivers.csv`, `races.csv`, `driver_standings.csv`, `pit_stops.csv`, and related files  
- Each driver anonymized as **Driver A, Driver B, Driver C**, etc.

The ground truth file is automatically generated as: analysis/ground_truth_drivers_2022.csv

---

## ğŸ§  Hypotheses
| ID | Hypothesis | Bias Type |
|----|-------------|-----------|
| H1 | LLM recommendations differ when describing a driver as â€œstrugglingâ€ vs â€œdeveloping.â€ | Framing |
| H2 | Mentioning driver nationality changes which driver is recommended for improvement. | Demographic |
| H3 | Asking â€œwhat went wrongâ€ vs â€œwhat opportunities existâ€ alters conclusions. | Framing |
| H4 | Priming with â€œFerrari has the strongest strategyâ€ increases model agreement. | Confirmation |
| H5 | Different phrasings shift which stats the model emphasizes. | Selection |

---

## âš™ï¸ Repository Structure

Task_08_Bias_Detection_F1/
â”‚
â”œâ”€â”€ src/                     # Core experiment scripts
â”‚   â”œâ”€â”€ experiment_design.py     # Builds prompts & ground truth
â”‚   â”œâ”€â”€ run_experiment.py        # Runs LLM queries (manual or API)
â”‚   â”œâ”€â”€ analyze_bias.py          # Sentiment & keyword analysis
â”‚   â”œâ”€â”€ validate_claims.py       # Checks LLM statements vs data
â”‚   â””â”€â”€ convert_manual_to_jsonl.py  # Converts manual logs for analysis
â”‚
â”œâ”€â”€ prompts/                 # Auto-generated prompt JSONs
â”œâ”€â”€ prompts_for_chat/        # Copyâ€“paste .md prompts for chat UIs
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ REPORT.md                # October 15 deliverable report
â”‚   â”œâ”€â”€ MANUAL_RUN.md            # Instructions for manual Gemini/Claude runs
â”‚   â”œâ”€â”€ OCT15_PLANNING_SUMMARY.md# Qualtrics-ready planning summary
â”‚   â””â”€â”€ All_Prompts_F1.docx      # Combined prompts for copy/paste
â”‚
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ figures/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README.md                # No raw data committed (anonymized only)
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ README.md                # Placeholder; real outputs excluded by .gitignore
â”‚
â”œâ”€â”€ config.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                    # (this file)


---

## ğŸ§ª How to Run (Manual Mode â€“ No APIs)

### Step 1 â€“ Generate Prompts
```bash
python src/experiment_design.py --season 2022 --out prompts
