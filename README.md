# Task_08_Bias_Detection

# ğŸï¸ Task 08 â€“ Bias Detection in LLM Data Narratives (F1 Dataset)

This repository implements a controlled experiment to detect **biases in Large Language Model (LLM)**â€“generated data narratives.  
It follows the structure of **Research Task 08** (Bias Detection in LLM Data Narratives) under Syracuse Universityâ€™s research framework.

---

## ğŸ“˜ Objective
To test whether identical datasets produce **different analyses from LLMs** when the **prompt framing** or **demographic context** changes.  
The experiment investigates:
- **Framing Bias:** â€œunderperformingâ€ vs â€œdevelopingâ€
- **Demographic Bias:** inclusion of nationalities or protected traits
- **Confirmation Bias:** testing primed hypotheses (e.g., â€œFerrari has the strongest strategyâ€)
- **Selection Bias:** which statistics the model emphasizes (wins, DNFs, pit stops, etc.)

---

## ğŸ§© Dataset
We use anonymized Formula 1 performance data from the **2022 season**, derived from:
- `drivers.csv`, `races.csv`, `driver_standings.csv`, `pit_stops.csv`, and related files  
- Each driver anonymized as **Driver A, Driver B, Driver C**, etc.

The ground truth file is automatically generated as: analysis/ground_truth_drivers_2022.csv

---

## ğŸ§  Hypotheses
| ID | Hypothesis | Bias Type |
|----|-------------|-----------|
| H1 | LLM recommendations differ when describing a driver as â€œstrugglingâ€ vs â€œdeveloping.â€ | Framing |
| H2 | Mentioning driver nationality changes which driver is recommended for improvement. | Demographic |
| H3 | Asking â€œwhat went wrongâ€ vs â€œwhat opportunities existâ€ alters conclusions. | Framing |
| H4 | Priming with â€œFerrari has the strongest strategyâ€ increases model agreement. | Confirmation |
| H5 | Different phrasings shift which stats the model emphasizes. | Selection |

---

## âš™ï¸ Repository Structure

Task_08_Bias_Detection_F1/
â”‚
â”œâ”€â”€ ğŸ“ src/                          # Core experiment scripts
â”‚   â”œâ”€â”€ experiment_design.py         # Builds anonymized F1 dataset & generates prompt sets
â”‚   â”œâ”€â”€ run_experiment.py            # Runs or simulates LLM outputs (manual/API)
â”‚   â”œâ”€â”€ analyze_bias.py              # Sentiment & keyword-based bias analysis
â”‚   â”œâ”€â”€ validate_claims.py           # Compares model statements to ground truth data
â”‚   â””â”€â”€ convert_manual_to_jsonl.py   # Converts manual chat logs (CSV) â†’ JSONL
â”‚
â”œâ”€â”€ ğŸ“ prompts/                      # JSON prompts for each bias condition
â”‚   â”œâ”€â”€ prompts_neutral_driver.json
â”‚   â”œâ”€â”€ prompts_positive_driver.json
â”‚   â”œâ”€â”€ prompts_negative_driver.json
â”‚   â”œâ”€â”€ prompts_neutral_driver_demo.json
â”‚   â”œâ”€â”€ prompts_positive_driver_demo.json
â”‚   â”œâ”€â”€ prompts_negative_driver_demo.json
â”‚   â””â”€â”€ prompts_prime_constructor.json
â”‚
â”œâ”€â”€ ğŸ“ prompts_for_chat/             # Copyâ€“paste-ready .md prompt files for Gemini & Claude
â”‚   â”œâ”€â”€ neutral_driver.md
â”‚   â”œâ”€â”€ negative_driver.md
â”‚   â”œâ”€â”€ positive_driver.md
â”‚   â”œâ”€â”€ prime_constructor.md
â”‚   â””â”€â”€ *_demo.md
â”‚
â”œâ”€â”€ ğŸ“ docs/                         # Reports & research documentation
â”‚   â”œâ”€â”€ REPORT.md                    # October 15 deliverable (executive summary & plan)
â”‚   â”œâ”€â”€ MANUAL_RUN.md                # Instructions for running via Gemini / Claude chat
â”‚   â”œâ”€â”€ OCT15_PLANNING_SUMMARY.md    # Qualtrics-ready summary for OPT reporting
â”‚   â”œâ”€â”€ All_Prompts_F1.docx          # All prompts combined for quick copy-paste
â”‚   â””â”€â”€ README.md                    # (Optional) extra project notes
â”‚
â”œâ”€â”€ ğŸ“ analysis/                     # Analytical outputs
â”‚   â”œâ”€â”€ ground_truth_drivers_2022.csv # Computed driver stats (points, pits, perf_score)
â”‚   â”œâ”€â”€ sentiment_by_condition_from_docx.csv
â”‚   â”œâ”€â”€ keywords_by_condition_from_docx.csv
â”‚   â”œâ”€â”€ validation_summary.csv
â”‚   â””â”€â”€ ğŸ“ figures/                  # Auto-generated visualizations
â”‚
â”œâ”€â”€ ğŸ“ data/                         # Raw CSVs (excluded from GitHub)
â”‚   â”œâ”€â”€ README.md                    # Placeholder; actual CSVs not committed
â”‚
â”œâ”€â”€ ğŸ“ results/                      # Model responses (excluded from GitHub)
â”‚   â”œâ”€â”€ manual_log_template.csv      # Template to record Gemini/Claude outputs
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ config.yaml                      # Config file for reproducibility
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore                       # Prevents committing raw data/results
â”œâ”€â”€ .github/workflows/lint.yml       # Optional CI workflow for linting
â””â”€â”€ README.md                        # Project overview & usage guide
---

## ğŸ§ª How to Run (Manual Mode â€“ No APIs)

### Step 1 â€“ Generate Prompts
```bash
python src/experiment_design.py --season 2022 --out prompts
